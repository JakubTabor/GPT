{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import torch"
      ],
      "metadata": {
        "id": "3y0APLuXxU3S"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Now we are gonna take a look at Multi-Head Attention\n",
        "# It is applying multiple attentions in parallel, then concatenating there results"
      ],
      "metadata": {
        "id": "WxlevY6Mt-96"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class BatchNorm1d:\n",
        "\n",
        "  def __init__(self, dim, eps=1e-5, momentum=0.1):\n",
        "    self.eps = eps\n",
        "    self.momentum = momentum\n",
        "    self.training = True\n",
        "    # Params trained with backprop\n",
        "    self.gamma = torch.ones(dim)\n",
        "    self.beta = torch.zeros(dim)\n",
        "    # Buffers trained with a running momentum update\n",
        "    self.running_mean = torch.zeros(dim)\n",
        "    self.running_var = torch.ones(dim)\n",
        "\n",
        "  def __call__(self, x):\n",
        "    # Calculate the forward pass\n",
        "    if self.training:\n",
        "      if x.ndim == 2:\n",
        "        dim = 0\n",
        "      elif x.ndim == 3:\n",
        "        dim = (0, 1)\n",
        "      xmean = x.mean(dim, keepdim=True)\n",
        "      xvar = x.var(dim, keepdim=True)\n",
        "    else:\n",
        "      xmean = self.running_mean\n",
        "      xvar = self.running_var\n",
        "    xhat = (x - xmean) / torch.sqrt(xvar + self.eps)\n",
        "    self.out = self.gamma * xhat + self.beta\n",
        "    # Update the buffers\n",
        "    if self.training:\n",
        "      with torch.no_grad():\n",
        "        self.running_mean = (1 - self.momentum) * self.running_mean + self.momentum * xmean\n",
        "        self.running_var = (1 - self.momentum) * self.running_var + self.momentum * xvar\n",
        "    return self.out\n",
        "\n",
        "  def parameters(self):\n",
        "    return [self.gamma, self.beta]\n",
        "\n",
        "torch.manual_seed(1337)\n",
        "module = BatchNorm1d(100)\n",
        "x = torch.randn(32, 100)\n",
        "x = module(x)\n",
        "x.shape\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uwRdUnMeyoBK",
        "outputId": "d7ecaaf6-7216-463e-99b7-f06cdb50ae6d"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([32, 100])"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Once we look at zero-th column its zero mean - tensor(7.4506e-09) and one standard deviation - tensor(1.0000),\n",
        "# So it's normalizing every single column of this input\n",
        "* But the rows are not normalized by default - tensor(0.0411), so we need to implement the layer norm"
      ],
      "metadata": {
        "id": "HMQqQ8OXyGZ7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x[:, 0].mean(), x[:, 0].std()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6bSmDuVTxcSD",
        "outputId": "44a47dbd-2e4e-4620-8f15-a173b8558be9"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor(7.4506e-09), tensor(1.0000))"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x[0, :].mean(), x[0, :].std()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q3OZmZaUzJJF",
        "outputId": "781c8ae0-f2bf-4ebd-c123-5a30df15a9a0"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor(0.0411), tensor(1.0431))"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "_jb5H-_i7Xj1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6a7412bc-6982-4875-b9ee-482a84d339c4"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([32, 100])"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ],
      "source": [
        "class LayerNorm:\n",
        "\n",
        "  def __init__(self, dim, eps=1e-5, momentum=0.1):\n",
        "    self.eps = eps\n",
        "    self.gamma = torch.ones(dim)\n",
        "    self.beta = torch.zeros(dim)\n",
        "\n",
        "  def __call__(self, x):\n",
        "    xmean = x.mean(1, keepdim=True)\n",
        "    xvar = x.var(1, keepdim=True)\n",
        "    xhat = (x - xmean) / torch.sqrt(xvar + self.eps)\n",
        "    self.out = self.gamma * xhat + self.beta\n",
        "    return self.out\n",
        "\n",
        "  def __parameters__(self):\n",
        "    return [self.gamma, self.beta]\n",
        "\n",
        "torch.manual_seed(1337)\n",
        "module = LayerNorm(100)\n",
        "x = torch.randn(32, 100)\n",
        "x = module(x)\n",
        "x.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# And now columns are not going to be normalized, but rows are normalized"
      ],
      "metadata": {
        "id": "ow73Yyjg2HwC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x[:, 0].mean(), x[:, 0].std()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y8rUw_nT17TM",
        "outputId": "8b9148c3-05fc-49df-bbc7-85773ee7ee3a"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor(0.1469), tensor(0.8803))"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x[0, :].mean(), x[0, :].std()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O3pNwVDMxipd",
        "outputId": "bd783dcc-d2df-482a-e2b2-31f33f1fc860"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor(-9.5367e-09), tensor(1.0000))"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# I introduce Layer-Norm, which is similar to Batch Normalization, so we can borrow the code from the Batch-Norm\n",
        "* We just do normalization across rows, not columns\n",
        "* Also we don't distinguished between training and test, we also don't need buffers\n",
        "* Because the computation do not span across the examples\n",
        "\n",
        "# Size of Layer-Norm is n_embed, so 32\n",
        "* When Layer-Norm is normalizing our features, the mean and variance is taken over 32 numbers\n",
        "* So batch and time act as batch-dimensions, this is like per token transformation, that normalizes the features\n",
        "* And make them unit mean and unit gaussian at initialization"
      ],
      "metadata": {
        "id": "UnBsgdWMxtNy"
      }
    }
  ]
}