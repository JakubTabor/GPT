{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch.nn import functional as F\n",
        "import torch.nn as nn"
      ],
      "metadata": {
        "id": "xDSf5Xl22oie"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(1337)\n",
        "B, T, C = 4, 8, 32\n",
        "x = torch.randn(B, T, C)\n",
        "\n",
        "head_size = 16\n",
        "key = nn.Linear(C, head_size, bias=False)\n",
        "query = nn.Linear(C, head_size, bias=False)\n",
        "value = nn.Linear(C, head_size, bias=False)\n",
        "k = key(x)\n",
        "q = query(x)\n",
        "wei = q @ k.transpose(-2, -1)\n",
        "\n",
        "tril = torch.tril(torch.ones(T, T))\n",
        "wei = wei.masked_fill(tril == 0, float('-inf'))\n",
        "wei = F.softmax(wei, dim=-1)\n",
        "\n",
        "v = value(x)\n",
        "out = wei @ v\n",
        "\n",
        "out.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WxixIWrs2vuu",
        "outputId": "c3c597bb-4d7a-4716-f6c6-289d0d39681f"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([4, 8, 16])"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Now we can look closer to scaled attention, we have query, key and value\n",
        "* We multiply query and key, next we softmax it and aggregate the values\n",
        "* To make scaled attention we use one more thing, we need to devide by one over âˆš of the head size (dk)\n",
        "* We do this, because"
      ],
      "metadata": {
        "id": "lSeRclhl2AHm"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "MWHt7eVi1rXw"
      },
      "outputs": [],
      "source": [
        "k = torch.randn(B, T, head_size)\n",
        "q = torch.randn(B, T, head_size)\n",
        "wei = q @ k.transpose(-2, -1)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# The problem comes when we have unit gaussian inputs\n",
        "* So k and q are unit gaussian, when we do wei without scaling, then we see that the wei variance will be on the order of head size (16)"
      ],
      "metadata": {
        "id": "40Y6i4I42cQu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "k.var()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uznBKqOO2f5t",
        "outputId": "1d0ca084-fa23-46c6-bb3e-cc79646a12ab"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(1.0449)"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "q.var()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A5W31I9p2gPd",
        "outputId": "ff065b71-c6cc-4bd9-ffa4-1ccf11b2e249"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(1.0700)"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "wei.var()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XIkrRlRU2gTG",
        "outputId": "2790955b-6f61-4f9f-b181-60117abc5957"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(17.4690)"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# But when we do the scaling, variance will be 1, so will be preserved"
      ],
      "metadata": {
        "id": "R1BnovVB2gfO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "wei = q @ k.transpose(-2 , -1) * head_size**-0.5"
      ],
      "metadata": {
        "id": "pvGBVSex5XA3"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "wei.var()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nkhlE0UW5ijt",
        "outputId": "99911164-cc19-4e8d-b5c6-d8ede9cef91e"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(1.0918)"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# This is important, because wei will feed into softmax\n",
        "# Its important especially at initialization, that the wei be fairy defused\n",
        "* But if wei takes very positive and negative numbers inside it, the softmax will converge towards one-hot vectors\n",
        "* Once we are applying softmax to values that are very close to zero, then we are gonna diffuse thing out of softmax"
      ],
      "metadata": {
        "id": "HvC3NDCX5PMm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "torch.softmax(torch.tensor([0.1, -0.2, 0.3, -0.2, 0.5]), dim=-1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HL2lN2Hq5-ol",
        "outputId": "a8ff4627-c890-4316-d1e8-491730250886"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([0.1925, 0.1426, 0.2351, 0.1426, 0.2872])"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# But once we do exact same thing, but start sharpening it, making it bigger by multiplying them by 8\n",
        "* It will sharpen all numbers towards the max (the highest number)\n",
        "* We don't want this values to be too extreme, especially at initialization, otherwise the softmax will be way to peaky\n",
        "* So we don't want to aggregate the informations from a single node, because every node then aggregate informations from this single other node\n",
        "\n",
        "# So scaling is used to control the variance at initialization"
      ],
      "metadata": {
        "id": "2pluR9uL6VGe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "torch.softmax(torch.tensor([0.1, -0.2, 0.3, -0.2, 0.5])*8, dim=-1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jYIvvaGF6V8m",
        "outputId": "e45e3d44-4159-4530-cb36-a046c50ba707"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([0.0326, 0.0030, 0.1615, 0.0030, 0.8000])"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Now we implemented scaled Dot-Product Attention"
      ],
      "metadata": {
        "id": "MOF5baFr6upv"
      }
    }
  ]
}