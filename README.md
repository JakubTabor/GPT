![](https://github.com/JakubTabor/GPT/blob/main/Images/Nano_GPT/Nano_GPT.png)

# Position encoding
![](https://github.com/JakubTabor/GPT/blob/main/Images/Nano_GPT/Position_encoding.png)

# Position embedding part
![](https://github.com/JakubTabor/GPT/blob/main/Images/Nano_GPT/text_pos_embed.png)
![](https://github.com/JakubTabor/GPT/blob/main/Images/Nano_GPT/Position_embeding.png)

# Multi-head Self-attention and LayerNorm
![](https://github.com/JakubTabor/GPT/blob/main/Images/Nano_GPT/LayerNorm_and_multiHeadAttention.png)
# Multi-head Self-attention in python code
![](https://github.com/JakubTabor/GPT/blob/main/Images/Nano_GPT/MultiHeadAttention_Py.png)
# LayerNorm in python as a raw code
![](https://github.com/JakubTabor/GPT/blob/main/Images/Nano_GPT/LayerNorm_Py.png)
# LayerNorm in use
![](https://github.com/JakubTabor/GPT/blob/main/Images/Nano_GPT/Block_Py.png)
